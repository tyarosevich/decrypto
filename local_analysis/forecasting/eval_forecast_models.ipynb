{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4481be85645116dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T23:26:43.791567Z",
     "start_time": "2024-04-11T23:26:43.654305Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "data_folder = Path('./../../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57843d8",
   "metadata": {},
   "source": [
    "## (I) Load Data (Crypto / Tweets/ Market)\n",
    "Some coins other than BTC and ETH are included. Future experimentation could involve using a large number of coins and performing separate feature extraction on that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111e4a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['BTC', 'ETH', 'SOL', 'LINK', 'USDC'])\n",
      "Index(['unix', 'date', 'symbol', 'open', 'high', 'low', 'close', 'volto',\n",
      "       'volfrom', 'close_ETH', 'close_SOL', 'close_LINK', 'close_USDC',\n",
      "       'open_SP500', 'high_SP500', 'low_SP500', 'close_SP500', 'volume_SP500',\n",
      "       'open_NSDQ_x', 'high_NSDQ_x', 'low_NSDQ_x', 'close_NSDQ_x',\n",
      "       'volume_NSDQ_x', 'open_NSDQ_y', 'high_NSDQ_y', 'low_NSDQ_y',\n",
      "       'close_NSDQ_y', 'volume_NSDQ_y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "coin_names = ['BTC', 'ETH', 'SOL', 'LINK', 'USDC']\n",
    "dct_coin_tables = {}\n",
    "for suffix in coin_names:\n",
    "    dct_coin_tables[suffix] = pd.read_csv(Path(data_folder / 'coin_index_vals_merged_{}.csv'.format(suffix)), parse_dates=['date'])\n",
    "\n",
    "print(dct_coin_tables.keys())\n",
    "\n",
    "df_bitcoin = dct_coin_tables['BTC'] # A view to a table for inspection because VScode Jupyter dict inspection is a catastrophe.\n",
    "print(df_bitcoin.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa8c81eff38277",
   "metadata": {},
   "source": [
    "The tweet data includes raw tweets, sentiment values from an up-to-date roBERTa model, and smaller tokens (64) from that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6555843f3e83abb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T23:26:46.592971Z",
     "start_time": "2024-04-11T23:26:46.234171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment shape: (5261255,) \n",
      "Token Shape: (5261255, 64) \n",
      "Tweet Shape: (5261255, 8)\n"
     ]
    }
   ],
   "source": [
    "tweet_sent_file_csv = Path('data/tweets_and_sent.csv')\n",
    "\n",
    "# Tweets need to be sorted appropriately, as I forgot to do it in the original processing.\n",
    "# TODO: Reprocess as sorted, I don't like it.\n",
    "df_tweets = pd.read_csv(tweet_sent_file_csv, parse_dates=['date'])\n",
    "df_tweets.reset_index(drop=True, inplace=True) # Not sure if I did this originally.\n",
    "\n",
    "print(\"Tweet and sentiment shape: {}\".format(df_tweets.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58693ec2",
   "metadata": {},
   "source": [
    "The crypto data outside the date-range of the tweets is dropped. In theory, this doesn't really matter, but it makes the dataset smaller for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d32838322d817bee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T23:24:53.259547Z",
     "start_time": "2024-04-11T23:24:53.257611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24071\n"
     ]
    }
   ],
   "source": [
    "assert(df_bitcoin['date'].dtype == df_tweets['date'].dtype)\n",
    "date_start = df_tweets['date'].min()\n",
    "date_end = df_tweets['date'].max()\n",
    "\n",
    "# Note the date mask needs to be kept around for some models and not others. In some cases, e.g. TFT, I believe the\n",
    "# tweet data can be nan-filled and used with the full crypto dataset. In others, e.g. ARIMA, I believe all multivariate\n",
    "# features need to be present.\n",
    "mask_dates = (df_bitcoin['date'] >= date_start) & (df_bitcoin['date'] <= date_end)\n",
    "mask_dates = np.where(mask_dates.to_numpy())[0]\n",
    "print(len(mask_dates))\n",
    "# This is a view, be mindful of indices.\n",
    "df_bitcoin_post_tweets = df_bitcoin.iloc[mask_dates, :]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "I opted to discard neutral tweets in order to have a single numerical sentiment feature for viz and analysis.\n",
    "In practice, this probably doesn't matter since so few tweets are neutral, and we believe a tweet is neutral, then we \n",
    "probably don't care about it anyway."
   ],
   "id": "8de0ce627cc6fed1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327bb116af65cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that neutral tweets are fairly unusual, and if we believe in the hypothesis that tweet sentiment reflects market\n",
    "# confidence, then neutral tweets don't contain actionable information anyway. Furthermore, neutral is ambiguous: a false\n",
    "# neutral's true sentiment can be either positive or negative.\n",
    "positive_cnt = np.sum(df_tweets['label'] == 'positive')\n",
    "negative_cnt = np.sum(df_tweets['label'] == 'negative')\n",
    "neutral_cnt = np.sum(df_tweets['label'] == 'neutral')\n",
    "print('Tweets %:\\nPositive: {}\\nNegative: {}\\nNeutral: {}'.format(positive_cnt/df_tweets.shape[0], negative_cnt/df_tweets.shape[0], neutral_cnt/df_tweets.shape[0]))\n",
    "\n",
    "\n",
    "sent_map = {'negative': -1, 'neutral': 0, 'positive': 1}\n",
    "df_tweets['single_val_sent'] = 0\n",
    "df_tweets['single_val_sent'] = df_tweets['label'].map(sent_map)\n",
    "# Might be a better way to do this that captures neutrality, but for monotonicity, this should be good enough for\n",
    "# cursory analysis.\n",
    "df_tweets['single_val_sent'] = df_tweets['single_val_sent'] * df_tweets['score']\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In preparation for feature extraction, a list of masks is created for the tweets dataframe. Each item in the date_pair_masks filters the tweets that correspond to a particular one-hour row of the bitcoin data.\n",
    "\n",
    "This was done with shared memory and multiprocessing in order to parallelize without copying the large'ish date array. This wasn't hugely necessary (only a 2 minute speedup), but serves as a warmup for feature extraction. Note that the dates had to be changed to posix time, since the shared memory really didn't seem to like datetime objects in the numpy array. Something to do with contiguous memory maybe."
   ],
   "id": "825e03650f4e568"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import multiprocessing as mp\n",
    "import functools\n",
    "import local_analysis.forecasting.utils_multiprocess as utils_multiprocess\n",
    "num_cores = mp.cpu_count() - 2\n",
    "\n",
    "# Convert bitcoin timestamps to posix, and get pairs of one hour minus 1s timespans for each row.\n",
    "btc_dates = df_bitcoin_post_tweets['date'].astype(int) / 10 ** 9\n",
    "btc_dates = btc_dates.astype(int).to_numpy()\n",
    "date_pairs = list(zip(btc_dates[0:-2], btc_dates[1:] - 1))\n",
    "\n",
    "tweet_dates = df_tweets['date'].astype(int) / 10 ** 9\n",
    "tweet_dates = tweet_dates.astype(int).to_numpy()\n",
    "\n",
    "NP_SHARED_NAME = 'npshared'\n",
    "try:\n",
    "    shm = utils_multiprocess.create_shared_memory_nparray(tweet_dates)\n",
    "except FileExistsError:\n",
    "    utils_multiprocess.release_shared(NP_SHARED_NAME)\n",
    "    shm = utils_multiprocess.create_shared_memory_nparray(tweet_dates)\n",
    "\n",
    "runner = functools.partial(utils_multiprocess.get_date_mask, array_shape = tweet_dates.shape)\n",
    "\n",
    "start = time()\n",
    "with mp.Pool(processes=num_cores) as pool:\n",
    "    processed = pool.map(runner, date_pairs)\n",
    "end = time()\n",
    "print('Time taken to create date_pairs with multiprocess: {}'.format(end - start))\n",
    "utils_multiprocess.release_shared(NP_SHARED_NAME)"
   ],
   "id": "66e8683b8b6b80e8"
  },
  {
   "cell_type": "markdown",
   "id": "8c38d21b",
   "metadata": {},
   "source": [
    "NOTE: Do an ACF plot and correlation matrix of the coin value and the sentiment. Check with lags. Note that ACF requires stationarity, so need to test for that. Also might want to just switch to return rate instead anyway.\n",
    "https://www.kaggle.com/code/iamleonie/time-series-interpreting-acf-and-pacf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d816c9e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
